{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm, trange\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from transformers import AutoTokenizer,AutoModelForTokenClassification, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = pd.read_csv(\"NER_data_marefa.csv\")\n",
    "input_data = input_data[input_data['sentence_id'] <=8]\n",
    "tags = [\"O\", \"job\", \"nationality\", \"person\", \"location\",\"time\", \"event\", \"organization\",\"product\", \"artwork\"]\n",
    "label_map = { v:index for index, v in enumerate(sorted(tags))}\n",
    "map_label = { index:v for index, v in enumerate(sorted(tags))}\n",
    "\n",
    "max_len = 250\n",
    "batch_size = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"marefa-nlp/marefa-ner\")\n",
    "function = lambda s: [s['sentence'].unique()[0], [label_map[i] for i in s[\"tag\"].values.tolist()]]\n",
    "grouped = input_data.groupby(\"sentence_id\").apply(function)\n",
    "items = [s for s in grouped]\n",
    "sents = [item[0] for item in items]\n",
    "labels = [item[1] for item in items]\n",
    "tokenized_texts = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sent)) for sent in sents]\n",
    "X = pad_sequences(tokenized_texts,maxlen=max_len, value=tokenizer.pad_token_id, padding=\"post\",dtype=\"long\", truncating=\"post\")\n",
    "Y = pad_sequences(labels,maxlen=max_len, value=label_map[\"O\"], padding=\"post\",dtype=\"long\", truncating=\"post\")\n",
    "#Y = to_categorical(Y, num_classes = len(tags))\n",
    "attention_masks = np.array([[float(i!=1) for i in ii] for ii in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8, 250), (8, 250), (8, 250))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape , Y.shape , attention_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X, Y,test_size=0.1)\n",
    "Mask_train, Mask_valid, _, _ = train_test_split(attention_masks, X,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train)\n",
    "X_valid = torch.tensor(X_valid)\n",
    "Y_train = torch.tensor(Y_train, dtype=torch.long)\n",
    "Y_valid = torch.tensor(Y_valid, dtype=torch.long)\n",
    "Mask_train = torch.tensor(Mask_train)\n",
    "Mask_valid = torch.tensor(Mask_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = TensorDataset(X_train, Mask_train, Y_train)\n",
    "data_train_sampler = RandomSampler(data_train)\n",
    "DL_train = DataLoader(data_train, sampler=data_train_sampler, batch_size=batch_size)\n",
    "\n",
    "data_valid = TensorDataset(X_valid, Mask_valid, Y_valid)\n",
    "data_valid_sampler = SequentialSampler(data_valid)\n",
    "DL_valid = DataLoader(data_valid, sampler=data_valid_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NER_model(nn.Module):\n",
    "    def __init__(self,num_classes = 10):\n",
    "        super(NER_model, self).__init__()\n",
    "        \n",
    "        config = AutoConfig.from_pretrained(\"marefa-nlp/marefa-ner\")\n",
    "        config.num_labels = num_classes\n",
    "        self.bert = AutoModelForTokenClassification.from_config(config)\n",
    "        ### New layers:\n",
    "        self.linear = nn.Softmax(dim = -1)\n",
    "        \n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        \n",
    "        sequence_output = self.bert(input_ids=ids, attention_mask=mask)\n",
    "        sequence_output = sequence_output.logits\n",
    "        logits = self.linear(sequence_output)\n",
    "        #y_hat = logits.argmax(-1)\n",
    "        return logits#, y_hat\n",
    "\n",
    "model = NER_model(num_classes = len(label_map))\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels, mask):\n",
    "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat[mask] == labels_flat[mask]) / len(labels_flat[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 7/7 [01:23<00:00, 11.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.7975263254983085\n",
      "Validation loss: 1.6638537645339966\n",
      "Validation Accuracy: 0.7972972972972973\n",
      "precision: 0.235 recall: 0.250 f1: 0.242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KUSHBAB\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    # TRAIN loop\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for i, batch in tqdm(enumerate(DL_train), total=len(DL_train)):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        b_input_ids = b_input_ids.type(torch.LongTensor) \n",
    "        b_input_mask = b_input_mask.type(torch.LongTensor)\n",
    "        b_labels = b_labels.type(torch.LongTensor)\n",
    "\n",
    "        b_input_ids = b_input_ids.to(device)\n",
    "        b_input_mask = b_input_mask.to(device)\n",
    "        b_labels = b_labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        logits = model(b_input_ids,mask=b_input_mask)\n",
    "        \n",
    "        active_loss = b_input_mask.view(-1) == 1\n",
    "        active_logits = logits.view(-1, logits.shape[-1])[active_loss]\n",
    "        active_labels = b_labels.view(-1)[active_loss]\n",
    "        loss = criterion(active_logits, active_labels)\n",
    "        \n",
    "        #loss = criterion(logits.view(-1, logits.shape[-1]), b_labels.view(-1))\n",
    "        loss.backward()\n",
    "        \n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    # VALIDATION on validation set\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions , true_labels = [], []\n",
    "    for batch in DL_valid:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        b_input_ids = b_input_ids.type(torch.LongTensor) \n",
    "        b_input_mask = b_input_mask.type(torch.LongTensor)\n",
    "        b_labels = b_labels.type(torch.LongTensor)\n",
    "\n",
    "        b_input_ids = b_input_ids.to(device)\n",
    "        b_input_mask = b_input_mask.to(device)\n",
    "        b_labels = b_labels.to(device)\n",
    "        active_loss=None\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids,mask=b_input_mask)\n",
    "            active_loss = b_input_mask.view(-1) == 1\n",
    "            active_logits = logits.view(-1, logits.shape[-1])[active_loss]\n",
    "            active_labels = b_labels.view(-1)[active_loss]\n",
    "            tmp_eval_loss = criterion(active_logits, active_labels)\n",
    "            #tmp_eval_loss = criterion(logits.view(-1, logits.shape[-1]), b_labels.view(-1))\n",
    "        \n",
    "        logits = logits.cpu().detach().numpy()\n",
    "        label_ids = b_labels.cpu().detach().numpy()\n",
    "        \n",
    "        predictions.extend([list(p) for p in np.argmax(logits, axis=-1)])\n",
    "        true_labels.append(label_ids)\n",
    "        \n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids, active_loss.cpu().detach().numpy())\n",
    "        \n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        \n",
    "        nb_eval_examples += b_input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "    eval_loss = eval_loss/nb_eval_steps\n",
    "    print(\"Validation loss: {}\".format(eval_loss))\n",
    "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "    pred_tags = [[map_label[p_i] for p_i in p] for p in predictions]\n",
    "    valid_tags = [[map_label[l_ii] for l_ii in l_i] for l in true_labels for l_i in l]\n",
    "    precision = precision_score(valid_tags[0],pred_tags[0],average=\"macro\")\n",
    "    recall =  recall_score(valid_tags[0],pred_tags[0],average=\"macro\")\n",
    "    f1 = f1_score(valid_tags[0],pred_tags[0],average=\"macro\")\n",
    "    print(\"precision: %.3f recall: %.3f f1: %.3f\" % (precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocesspred(sent,tags,scores,grouped):\n",
    "    if grouped:\n",
    "        a = sent.split()\n",
    "        b = tokenizer.tokenize(sent)\n",
    "        i = -1\n",
    "        j = 0\n",
    "        d = [-1]*len(a)\n",
    "        e = [0.0]*len(a)\n",
    "        f = [0]*len(a)\n",
    "        while True:\n",
    "            if b[j][0] == '▁':\n",
    "                i+=1\n",
    "                d[i] = tags[j]\n",
    "                e[i] += scores[j]\n",
    "                f[i] += 1\n",
    "            j+=1\n",
    "            if -1 not in d and (j>=len(b) or i>=len(a)):\n",
    "                break\n",
    "        g = np.array(e)/np.array(f)\n",
    "        array = []\n",
    "        words = tokenizer.tokenize(sent)\n",
    "        for l,w,s in zip(d,a,g):\n",
    "            array.append([w,map_label[l],s])\n",
    "        return array\n",
    "        return d\n",
    "    else:\n",
    "        array = []\n",
    "        words = tokenizer.tokenize(sent)\n",
    "        for l,w,s in zip(tags,words,scores):\n",
    "            array.append([w,map_label[l],s])\n",
    "        return array\n",
    "\n",
    "def predict(texts,grouped=False):\n",
    "    if isinstance(texts,str):\n",
    "        texts = [texts]\n",
    "    array = []\n",
    "    for text in texts:\n",
    "        x = pad_sequences([tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))],maxlen=max_len, value=tokenizer.pad_token_id, padding=\"post\",dtype=\"long\", truncating=\"post\")\n",
    "        mask = np.array([[float(i!=1) for i in ii] for ii in x])\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            model.to(device)\n",
    "            predictions = model(torch.tensor(x, dtype=torch.long), mask=torch.tensor(mask, dtype=torch.long))\n",
    "            predictions = predictions.cpu().numpy()\n",
    "            mask = mask.astype('int32')\n",
    "            scores = np.max(predictions,-1).reshape(predictions.shape[0]*predictions.shape[1],)[np.where(mask.reshape(mask.shape[0]*mask.shape[1],) == 1)]\n",
    "            predictions = np.argmax(predictions,-1).reshape(predictions.shape[0]*predictions.shape[1],)[np.where(mask.reshape(mask.shape[0]*mask.shape[1],) == 1)]\n",
    "            array.append(postprocesspred(text,predictions,scores,grouped))\n",
    "    if len(array) == 1:\n",
    "        return array[0]\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['▁ت', 'O', 0.99999976], ['تمتع', 'O', 0.99999976], ['▁مصر', 'O', 0.99999976], ['▁بعد', 'O', 0.99999976], ['د', 'O', 0.99999976], ['▁من', 'O', 0.99999976], ['▁الم', 'O', 0.99999976], ['زار', 'O', 0.99999976], ['ات', 'O', 0.99999976], ['▁السياحية', 'O', 0.99999976]]\n"
     ]
    }
   ],
   "source": [
    "a = predict(\"تتمتع مصر بعدد من المزارات السياحية\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['تتمتع', 'O', 0.9999997615814209], ['مصر', 'O', 0.9999997615814209], ['بعدد', 'O', 0.9999997615814209], ['من', 'O', 0.9999997615814209], ['المزارات', 'O', 0.9999997615814209], ['السياحية', 'O', 0.9999997615814209]]\n"
     ]
    }
   ],
   "source": [
    "a = predict(\"تتمتع مصر بعدد من المزارات السياحية\",True)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
